{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"Turquoise\"><b>Práctica 3: Introducción al Clustering</b></font>\n",
        "\n",
        "Elaborado con el apoyo de:\n",
        "Luis Fernando Becerra, BEDA Aprendizaje de Máquinas 2024-1S - 2025-1S\n",
        "Andres Esteban Marin Manco, BEDA Aprendizaje de Máquinas 2025-1S\n",
        "\n",
        "# <font color=\"LightPink\"><b>Clustering - Parte I</b></font>\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://dezyre.gumlet.io/images/blog/clustering-algorithms-in-machine-learning/Segmenting_Data_with_Clustering_Algorithms.png?w=376&dpr=2.6\" width=\"400\"/>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "En esta primera parte, vamos a utilizar los algoritmos de Kmeans, aglomerativo, DBSCAN y Spectral Clustering en un conjunto de datos de juguete, para analizar las ventajas y desventajas de cada uno de los métodos, efectos de inicialización y selección de parámetros"
      ],
      "metadata": {
        "id": "2FLMLMpQ1hG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"LightPink\"><b>1. Generación de Datos Sintéticos</b></font>\n",
        "\n",
        "Utilizaremos funciones de `sklearn.datasets` para crear conjuntos de datos con diferentes formas geométricas, lo cual nos ayudará a observar cómo los algoritmos se comportan con distribuciones variadas.\n",
        "\n",
        "- `make_blobs`: genera nube de puntos con distribución gaussiana (clusters esféricos). Por defecto, generar 3.\n",
        "- `make_moons`: genera un conjunto de datos distirbuidos en dos semicirculos.\n",
        "- `make_circles`: genera un conjunto de datos distribuidos en dos circulos (uno dentro del otro) en dos dimensiones."
      ],
      "metadata": {
        "id": "qSpctmSD2VmE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPpE6YTnrcdC"
      },
      "outputs": [],
      "source": [
        "#Importar librerias\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import cluster, datasets\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generamos 500 muestras para cada dataset\n",
        "\n",
        "#Nubes de puntos gaussianos (3 clusters)\n",
        "\n",
        "#Circulos concentricos (2 clusters)\n",
        "\n",
        "#Semicirculos (2 clusters)\n",
        "\n",
        "#Nubes de puntos gaussianos (10 clusters)\n",
        "\n"
      ],
      "metadata": {
        "id": "JZiY9XDzr5bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar Scaler\n",
        "\n",
        "#Grafica de los datasets\n",
        "colors = np.array([\"#377eb8\",\n",
        "                   \"#ff7f00\",\n",
        "                   \"#4daf4a\",\n",
        "                   \"#f781bf\",\n",
        "                   \"#a65628\",\n",
        "                   \"#984ea3\",\n",
        "                   \"#999999\",\n",
        "                   \"#e41a1c\",\n",
        "                   \"#dede00\",\n",
        "                   \"#008000\",\n",
        "                   \"#0343DF\",\n",
        "                   \"#7FFF00\",\n",
        "                   \"#ED0DD9\",\n",
        "                   \"#FBDD7E\",\n",
        "                   \"#FFA500\"])\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "data_sets = [(blobs),(noisy_circles),(noisy_moons),(blobs10)]\n",
        "i=1\n",
        "for dataset in data_sets:\n",
        "  ##\n",
        "  y = dataset[1]\n",
        "  plt.subplot(1,len(data_sets),i)\n",
        "  plt.scatter(X[:, 0], X[:, 1],color=colors[y])\n",
        "  i+=1"
      ],
      "metadata": {
        "id": "FohseQRft8_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"LightPink\"><b>K-Means</b></font>\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/NEONScience/NEON-Data-Skills/main/graphics/aop-spectral-py/classification/kmeans2d.gif\" width=\"500\"/>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "El término `K-Means` fue utilizado por primera vez por James MacQueen en 1967, aunque la idea se remonta a Hugo Steinhaus en 1956. El algoritmo estándar fue propuesto por primera vez por Stuart Lloyd, de los Laboratorios Bell, en 1957 como técnica para la modulación por impulsos codificados, aunque no se publicó como artículo en una revista hasta 1982.\n",
        "\n",
        "`K-Means` es el algoritmo de *clustering* más **popular** y **utilizado**. Se caracteriza por ser **fácil de implementar**, **computacionalmente rápido** y **escalable**. Además, permite ajustar la granularidad de los grupos generados modificando el número de clusters (`k`), lo que puede dar lugar a subagrupamientos.\n",
        "\n",
        "No obstante, su desempeño está limitado a **agrupamientos de tipo convexo** y requiere que el número de clusters (`k`) sea definido **previamente**. También es **sensible a la inicialización** de los centroides, lo que puede afectar la calidad de los resultados.\n",
        "\n",
        "Este algoritmo tiene aplicaciones en múltiples áreas, tales como:\n",
        "\n",
        "- **Geoestadística**\n",
        "- **Visión por computador**\n",
        "- **Segmentación de mercados**\n",
        "- **Estudios sísmicos**\n",
        "- **Clasificación de uso del suelo**\n"
      ],
      "metadata": {
        "id": "3QBQe4TvvoBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importar Kmeans\n"
      ],
      "metadata": {
        "id": "l0524EfbvqHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero vamos aplicarlo al dataset de nube de puntos con tres clusters. En esta primera parte del experimento vamos a asumir que conocemos el número de clusters."
      ],
      "metadata": {
        "id": "FgU6HC3U7TiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Organizamos los datos\n",
        "\n",
        "#Y: Solo por referencia, no se utilizan en el algoritmo\n",
        "\n",
        "#Configuramos los parametros\n",
        "#Ingresamos número de clusters 'n_clusters'\n",
        "#Seleccionamos la inicialización aleatoria\n",
        "#Definimos cuantas veces se repite el proceso n_init = 10 por defecto\n",
        "\n",
        "#Entrenamos el modelo\n",
        "\n",
        "#Obtenemos las etiquetas de pertenencia\n",
        "\n",
        "#Graficamos el resultado\n",
        "plt.scatter(X[:,0],X[:,1],color=colors[y_pred_kmeans])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TqnwyedBv5X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Una forma de evaluar el resultado es mediante la suma de las distancias al cuadrado de cada muestra al centroide de su cluster, este parametro se calcula durante el entrenamiento, y queda almacenado en el atributo inertia_.\n",
        "\n"
      ],
      "metadata": {
        "id": "p3ApOXNhFZxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mostrar la suma de la distancia al cuadrado al centroide\n"
      ],
      "metadata": {
        "id": "04sf1jTaFWVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1574910c-465e-4798-eaf2-51cab8728bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "148.51196245743648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos aplicar el metodo `K-Means` a los **cuatro conjuntos de datos sintéticos** generados previamente:\n",
        "\n"
      ],
      "metadata": {
        "id": "TjEF2VSl8XMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Organizamos los datos\n",
        "\n",
        "\n",
        "#El número de clusters a buscar en cada conjunto\n",
        "data_n = [3,2,2,10]\n",
        "plt.figure(figsize=(15,10))\n",
        "#Vamos a realizar un ciclo para agrupar cada conjunto de datos\n",
        "for i, dataset in enumerate(data_sets):\n",
        "  X = scaler.fit_transform(dataset[0])\n",
        "  y = dataset[1]\n",
        "  #Graficamos los datos originales\n",
        "  plt.subplot(2,len(data_sets),i+1)\n",
        "  plt.scatter(X[:,0],X[:,1],color=colors[y])\n",
        "  #Configuramos KMeans\n",
        "\n",
        "   #Entrenamos el modelo\n",
        "\n",
        "  #Extraemos las etiquetas y la suma de la distancias\n",
        "\n",
        "  #Graficamos los clusters finales\n",
        "  plt.subplot(2,len(data_sets),i+5)\n",
        "  plt.scatter(X[:,0],X[:,1],color=colors[y_pred_kmeans])"
      ],
      "metadata": {
        "id": "OSKL9akM8cNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nota importante: los colores solo permiten establecer la correspondencia a un mismo clusters, más no permite comparar entre los datos originales y los obtenidos con el algoritmo."
      ],
      "metadata": {
        "id": "MkqEpO8K-gM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mostramos la suma de distancias al cuadrado a los centroides\n"
      ],
      "metadata": {
        "id": "xzaPlscb7-9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"LightPink\"><b>Métodos Jerárquicos Aglomerativos</b></font>\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/0*848julTIRU1JZYft\" width=\"500\"/>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "Los métodos `jerárquicos aglomerativos` producen una serie anidada de particiones. La representación de los resultados se realiza mediante un `dendrograma`, el cual muestra la agrupación jerárquica de los patrones y los distintos niveles de *similitud* entre ellos.\n",
        "\n",
        "Aunque estos métodos **no requieren definir previamente el número de clusters**, sí es necesario **determinar un punto de corte en el dendrograma** para obtener la partición final. El enfoque aglomerativo se caracteriza por comenzar con **un cluster por cada muestra**, e ir **fusionando progresivamente** los clusters más similares hasta alcanzar la estructura deseada.\n",
        "\n",
        "Estos algoritmos son *conceptualmente simples* y presentan un **buen desempeño en conjuntos de datos pequeños**. Sin embargo, pueden tener dificultades cuando se enfrentan a **formas de clusters no convexas o de densidad variable**.\n",
        "\n",
        "Las aplicaciones de los métodos jerárquicos aglomerativos incluyen:\n",
        "\n",
        "- **Reconocimiento de patrones**\n",
        "- **Segmentación de imágenes**\n",
        "- **Redes de sensores inalámbricos**\n",
        "- **Planeación urbana**\n",
        "- **Análisis de datos espaciales**\n",
        "\n"
      ],
      "metadata": {
        "id": "5AQ9iLvggK8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importar libreria\n"
      ],
      "metadata": {
        "id": "p8Ch284LhOvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicialmente utilizaremos la **distancia euclidiana** con el método de **enlace simple** para construir el dendrograma y realizar la fusión de clusters.\n",
        "\n",
        "Existen diferentes tipos de enlace con los cuales se puede experimentar, cada uno con implicaciones distintas en la forma como se agrupan los datos:\n",
        "\n",
        "- **Simple**: utiliza la **distancia mínima** entre un par de observaciones de dos clusters.\n",
        "- **Completo**: utiliza la **distancia máxima** entre un par de observaciones de dos clusters.\n",
        "- **Promedio**: emplea la **distancia promedio** entre todos los pares posibles de observaciones entre dos clusters.\n",
        "- **Ward**: fusiona los clusters minimizando la **varianza total dentro de los grupos** resultantes.\n"
      ],
      "metadata": {
        "id": "1QUuGxebhV7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Organizamos los datos\n",
        "X = scaler.fit_transform(blobs[0])\n",
        "y = blobs[1] #Solo por referencia, no se utilizan en el algoritmo\n",
        "\n",
        "#Configuramos los parametros\n",
        "#Ingresamos número de clusters 'n_clusters'\n",
        "#Usamos euclidina y enlace simple\n",
        "\n",
        "\n",
        "#Entrenamos el modelo\n",
        "\n",
        "#Obtenemos las etiquetas de pertenencia\n",
        "\n",
        "#Graficamos el resultado\n",
        "plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_agglo])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lVT8IXq3iKXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparemos que ocurre con otros enlaces.\n",
        "Nota: para el enlace ward solo acepta la distancia euclidiana."
      ],
      "metadata": {
        "id": "_4WiUgRYjjAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enlace completo\n",
        "\n",
        "#Enlace promedio\n",
        "\n",
        "#Enlace Ward\n",
        "\n",
        "\n",
        "#Graficamos los clusters finales\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1,4,1)\n",
        "plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_agglo])\n",
        "plt.title('Single Linkage', size=18)\n",
        "plt.subplot(1,4,2)\n",
        "plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_aggloC])\n",
        "plt.title('Complete Linkage', size=18)\n",
        "plt.subplot(1,4,3)\n",
        "plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_aggloA])\n",
        "plt.title('Average Linkage', size=18)\n",
        "plt.subplot(1,4,4)\n",
        "plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_aggloW])\n",
        "plt.title('Ward Linkage', size=18)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kI_x-CkRjfxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos aplicar el metodo `Jerárquicos Aglomerativos` a los **cuatro conjuntos de datos sintéticos** generados previamente:\n",
        "\n"
      ],
      "metadata": {
        "id": "fwpMbZskk68l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Organizamos los datos\n",
        "data_sets = [(blobs),(noisy_circles),(noisy_moons),(blobs10)]\n",
        "type_linkage = ['single', 'complete', 'average', 'ward']\n",
        "#El número de clusters a buscar en cada conjunto\n",
        "data_n = [3,2,2,10]\n",
        "plt.figure(figsize=(15, 10))\n",
        "#Vamos a realizar un ciclo para agrupar cada conjunto de datos\n",
        "for i, dataset in enumerate(data_sets):\n",
        "  X = scaler.fit_transform(dataset[0])\n",
        "  y = dataset[1]\n",
        "  #Graficamos los datos originales\n",
        "  plt.subplot(5,len(data_sets),i+1)\n",
        "  plt.scatter(X[:, 0], X[:, 1],color=colors[y])\n",
        "  for j, link in enumerate(type_linkage):\n",
        "    #Configuramos, entrenamos y extraemos etiquetas\n",
        "\n",
        "\n",
        "    #Graficamos los clusters finales\n",
        "    plt.subplot(5,len(data_sets),i+5+j*4)\n",
        "    plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_CA])\n",
        "    plt.title(link, size=12)"
      ],
      "metadata": {
        "id": "ETDnUwd1k6W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"LightPink\"><b>DBSCAN</b></font>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://miro.medium.com/v2/resize:fit:888/0*WFeUIkj3-vYk0_c7\" width=\"300\"/>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "El algoritmo `DBSCAN` (*Density-Based Spatial Clustering of Applications with Noise*) fue propuesto por **Martin Ester**, **Hans-Peter Kriegel**, **Jörg Sander** y **Xiaowei Xu** en *1996*.\n",
        "\n",
        "Se trata de un algoritmo de agrupamiento basado en *densidad*, capaz de identificar **clusters de forma arbitraria** (incluso no convexas) y de distintos tamaños. `DBSCAN` puede detectar **clusters rodeados por otros clusters**, y es **robusto frente al ruido y a los datos atípicos**.\n",
        "\n",
        "A diferencia de métodos como `K-Means`, `DBSCAN` **no requiere especificar el número de clusters** a priori, pero sí depende de la correcta elección de dos parámetros: `eps` (radio de vecindad) y `min_samples` (mínimo de puntos por vecindad).\n",
        "\n",
        "Este algoritmo se ha utilizado en aplicaciones como:\n",
        "\n",
        "- **Revisión y análisis de literatura científica**\n",
        "- **Procesamiento de imágenes satelitales**\n",
        "- **Cristalografía de rayos X**\n",
        "- **Detección de anomalías o outliers**\n"
      ],
      "metadata": {
        "id": "gZAis3zev4YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importar libreria\n"
      ],
      "metadata": {
        "id": "f27F47Lswon6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicaremos el algoritmo `DBSCAN` al conjunto de datos generado con `make_blobs`, el cual contiene **tres clusters bien definidos** en forma de nubes de puntos.\n",
        "\n",
        "En esta primera prueba utilizaremos la **distancia euclidiana** como métrica por defecto y los siguientes parámetros:\n",
        "\n",
        "- `eps = 0.5`: radio del vecindario considerado para la densidad\n",
        "- `min_samples = 5`: número mínimo de puntos requeridos para formar un cluster\n",
        "\n"
      ],
      "metadata": {
        "id": "A8Ln2bENwBje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Organizamos los datos\n",
        "X = scaler.fit_transform(blobs[0])\n",
        "y = blobs[1] #Solo por referencia, no se utilizan en el algoritmo\n",
        "\n",
        "#Configuramos los parametros\n",
        "#Usamos euclidina\n",
        "\n",
        "#Entrenamos el modelo\n",
        "\n",
        "#Obtenemos las etiquetas de pertenencia\n",
        "\n",
        "#Graficamos el resultado\n",
        "plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_db])\n",
        "#Numero de clusters - eliminando noise points\n",
        "n_clusters_ = len(set(y_pred_db)) - (1 if -1 in y_pred_db else 0)\n",
        "n_noise_ = list(y_pred_db).count(-1)\n",
        "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
        "print(\"Estimated number of noise points: %d\" % n_noise_)"
      ],
      "metadata": {
        "id": "ETxnHz4DxDNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El algoritmo `DBSCAN` es **altamente sensible** a la elección de los parámetros `eps` y `min_samples`."
      ],
      "metadata": {
        "id": "jIZSo1u_xZ1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuramos los parametros\n",
        "\n",
        "#Entrenamos el modelo\n",
        "clustering.fit(X)\n",
        "#Obtenemos las etiquetas de pertenencia\n",
        "y_pred_db = clustering.labels_\n",
        "#Graficamos el resultado\n",
        "plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_db])\n",
        "#Numero de clusters - eliminando noise points\n",
        "n_clusters_ = len(set(y_pred_db)) - (1 if -1 in y_pred_db else 0)\n",
        "n_noise_ = list(y_pred_db).count(-1)\n",
        "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
        "print(\"Estimated number of noise points: %d\" % n_noise_)"
      ],
      "metadata": {
        "id": "nHKeN-jwxZnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos aplicar el metodo `DBSCAN` a los **cuatro conjuntos de datos sintéticos** generados previamente:\n",
        "\n"
      ],
      "metadata": {
        "id": "mKtlLR4N0DdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Organizamos los datos\n",
        "data_sets = [(blobs),(noisy_circles),(noisy_moons),(blobs10)]\n",
        "sum_dist = []\n",
        "#El número de clusters a buscar en cada conjunto\n",
        "data_n = [3,2,2,10]\n",
        "plt.figure(figsize=(15, 10))\n",
        "#Vamos a realizar un ciclo para agrupar cada conjunto de datos\n",
        "for i, dataset in enumerate(data_sets):\n",
        "  X = scaler.fit_transform(dataset[0])\n",
        "  y = dataset[1]\n",
        "  #Graficamos los datos originales\n",
        "  plt.subplot(2,len(data_sets),i+1)\n",
        "  plt.scatter(X[:, 0], X[:, 1],color=colors[y])\n",
        "  #Configuramos DBSCAN\n",
        "\n",
        "  #Entrenamos el modelo\n",
        "  clustering.fit(X)\n",
        "  #Extraemos las etiquetas\n",
        "  y_pred_DB = clustering.labels_\n",
        "  #Graficamos los clusters finales\n",
        "  plt.subplot(2,len(data_sets),i+5)\n",
        "  plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_DB])"
      ],
      "metadata": {
        "id": "8RyVTTDX0DEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"LightPink\"><b>Spectral Clustering</b></font>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://sandipanweb.wordpress.com/wp-content/uploads/2016/07/circles1.gif?w=676\" width=\"500\"/>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "El método `Spectral Clustering` utiliza el **espectro (valores propios)** de una `matriz de similaridad` para proyectar los datos en un espacio de menor dimensión y luego realizar el agrupamiento.\n",
        "\n",
        "A partir de una matriz de similaridad —que representa los datos como un `grafo`— se construye una `matriz laplaciana`. Sobre esta matriz se calculan los **k vectores propios más significativos**, y posteriormente se aplica el algoritmo `K-Means` en ese nuevo espacio reducido para obtener los grupos finales.\n",
        "\n",
        "Existen diversas definiciones del `Laplaciano del grafo`, dependiendo del tipo de normalización aplicada.\n",
        "\n",
        "Entre las aplicaciones destacadas de este enfoque se encuentra la **segmentación de imágenes**, debido a su capacidad para capturar relaciones complejas y estructuras no lineales en los datos.\n"
      ],
      "metadata": {
        "id": "kIBJm2zm3ll0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importar libreria\n"
      ],
      "metadata": {
        "id": "ufqwxQoi4Pb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero vamos a aplicar el algoritmo `Spectral Clustering` al dataset de nube de puntos con tres clusters.\n",
        "\n",
        "Entre los parámetros a incluir se encuentran:\n",
        "\n",
        "- `n_clusters`: obligatorio para este algoritmo.\n",
        "- `n_init`: número de veces que se ejecuta `K-Means` con diferentes semillas. Por defecto es `10`.\n",
        "- `affinity`: define la construcción de la matriz de afinidad. Por defecto es `'rbf'`, pero también puede usarse `'nearest_neighbors'`.\n",
        "\n",
        "Estos parámetros determinan cómo se forma la matriz laplaciana y cómo se realiza la agrupación final en el espacio reducido.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eR8wtZP24gnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Organizamos los datos\n",
        "X = scaler.fit_transform(blobs[0])\n",
        "y = blobs[1] #Solo por referencia, no se utilizan en el algoritmo\n",
        "\n",
        "#Configuramos los parametros\n",
        "\n",
        "#Entrenamos el modelo\n",
        "\n",
        "#Obtenemos las etiquetas de pertenencia\n",
        "\n",
        "#Graficamos el resultado\n",
        "plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_SC])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FVoNUzIn5HPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos aplicar el metodo `Spectral Clustering` a los **cuatro conjuntos de datos sintéticos** generados previamente:\n"
      ],
      "metadata": {
        "id": "Gcpsc8E85u2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Organizamos los datos\n",
        "data_sets = [(blobs),(noisy_circles),(noisy_moons),(blobs10)]\n",
        "sum_dist = []\n",
        "#El número de clusters a buscar en cada conjunto\n",
        "data_n = [3,2,2,10]\n",
        "plt.figure(figsize=(15, 10))\n",
        "#Vamos a realizar un ciclo para agrupar cada conjunto de datos\n",
        "for i, dataset in enumerate(data_sets):\n",
        "  X = scaler.fit_transform(dataset[0])\n",
        "  y = dataset[1]\n",
        "  #Graficamos los datos originales\n",
        "  plt.subplot(2,len(data_sets),i+1)\n",
        "  plt.scatter(X[:, 0], X[:, 1],color=colors[y])\n",
        "  #Configuramos Spectral Clustering\n",
        "  #clustering = SpectralClustering(n_clusters=data_n[i],n_init=10,affinity='rbf',gamma=1)\n",
        "\n",
        "   #Entrenamos el modelo\n",
        "\n",
        "   #Extraemos las etiquetas\n",
        "  y_pred_SC = clustering.labels_\n",
        "  #Graficamos los clusters finales\n",
        "  plt.subplot(2,len(data_sets),i+5)\n",
        "  plt.scatter(X[:, 0], X[:, 1],color=colors[y_pred_SC])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "30Ym3DH25uWt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}