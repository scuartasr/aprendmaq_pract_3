{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"Turquoise\"><b>Práctica 3: Número de clusters</b></font>\n",
        "\n",
        "Elaborado con el apoyo de:\n",
        "Luis Fernando Becerra, BEDA Aprendizaje de Máquinas 2024-1S - 2025-1S\n",
        "Andres Esteban Marin Manco, BEDA Aprendizaje de Máquinas 2025-1S\n",
        "\n",
        "\n",
        "# <font color=\"LightPink\"><b>Clustering - Parte II</b></font>\n",
        "\n",
        "En esta segunda parte vamos a explorar algunos mecanismos para **definir el número de clusters**, un paso crucial en tareas de agrupamiento.\n",
        "\n",
        "En aplicaciones reales, pocas veces se dispone de esta información de manera explícita. Por ello, es fundamental conocer herramientas que nos permitan **estimar una cantidad adecuada de grupos** a partir de los datos mismos.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2FLMLMpQ1hG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero usaremos el dataset de juguete tanto para 3 como para 10 clusters que creamos en la primera parte.\n",
        "\n"
      ],
      "metadata": {
        "id": "qSpctmSD2VmE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPpE6YTnrcdC"
      },
      "outputs": [],
      "source": [
        "#Importar librerias\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import cluster, datasets\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generamos 500 muestras para cada dataset\n",
        "n_samples = 500\n",
        "seed = 30\n",
        "#Nubes de puntos gaussianos (3 clusters)\n",
        "blobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\n",
        "#Nubes de puntos gaussianos (10 clusters)\n",
        "blobs10 = datasets.make_blobs(n_samples=n_samples, centers=10, random_state=seed)"
      ],
      "metadata": {
        "id": "JZiY9XDzr5bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Grafica de los datasets\n",
        "scaler = StandardScaler()\n",
        "colors = np.array([\"#377eb8\",\n",
        "                   \"#ff7f00\",\n",
        "                   \"#4daf4a\",\n",
        "                   \"#f781bf\",\n",
        "                   \"#a65628\",\n",
        "                   \"#984ea3\",\n",
        "                   \"#999999\",\n",
        "                   \"#e41a1c\",\n",
        "                   \"#dede00\",\n",
        "                   \"#008000\",\n",
        "                   \"#0343DF\",\n",
        "                   \"#7FFF00\",\n",
        "                   \"#ED0DD9\",\n",
        "                   \"#FBDD7E\",\n",
        "                   \"#FFA500\"])\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "data_sets = [(blobs),(blobs10)]\n",
        "i=1\n",
        "for dataset in data_sets:\n",
        "  X = scaler.fit_transform(dataset[0])\n",
        "  y = dataset[1]\n",
        "  plt.subplot(1,len(data_sets),i)\n",
        "  plt.scatter(X[:, 0], X[:, 1],color=colors[y])\n",
        "  i+=1"
      ],
      "metadata": {
        "id": "FohseQRft8_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"LightPink\"><b>K-Means</b></font>\n",
        "\n",
        "En el caso del algoritmo `K-Means`, una estrategia común para estimar el número adecuado de clusters consiste en analizar la **suma de distancias de cada punto a su centroide más cercano**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3QBQe4TvvoBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Método del codo**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/image-24.png\" width=\"350\"/>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "La métrica **inercia intra-cluster** tiende a disminuir a medida que se incrementa el número de clusters. Sin embargo, a partir de cierto punto, la reducción de la inercia se vuelve marginal. Este comportamiento da lugar al conocido **método del codo**, que ayuda a identificar un valor apropiado para `k`.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MuDEU_mWSSxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importar Kmeans\n",
        "from sklearn.cluster import KMeans\n",
        "#Organizamos los datos\n",
        "#X = scaler.fit_transform(blobs[0])\n",
        "X = scaler.fit_transform(blobs10[0])\n",
        "y = blobs[1]\n",
        "# arreglo para la suma de distancias\n",
        "\n",
        "\n",
        "#Graficamos el resultado\n"
      ],
      "metadata": {
        "id": "l0524EfbvqHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usualmente, se toma el **final del codo** como el valor estimado del número de clusters, aunque esta elección puede tener **diferentes interpretaciones** según el contexto y la naturaleza de los datos.\n",
        "\n",
        "\n",
        "### **Índice Davies-Bouldin y Silueta**\n",
        "\n",
        "A continuación, estudiaremos qué sucede al analizar algunos **índices de validación**, los cuales permiten evaluar la calidad del agrupamiento sin requerir etiquetas verdaderas:\n",
        "\n",
        "- **Davies-Bouldin**: propuesto en *1979*, se basa en la **similitud promedio de un cluster con su cluster más similar**. Valores **más bajos** indican un mejor agrupamiento.\n",
        "\n",
        "- **Silueta**: mide la **similitud de un punto con su propio cluster (cohesión)** en comparación con otros clusters (separación). El índice oscila entre `-1` y `+1`, donde un valor **alto** sugiere que los objetos están bien cohesionados dentro de su cluster y bien separados de los demás.\n",
        "\n"
      ],
      "metadata": {
        "id": "J4C4_w33E2iA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importamos el indice\n",
        "\n",
        "#Creamos un arreglo para almacenar los posibles valores\n",
        "\n",
        "#X = scaler.fit_transform(blobs[0])\n",
        "\n",
        "#Valor minimo de k debe ser 2 para db\n"
      ],
      "metadata": {
        "id": "2b7ChoZGE2Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importamos el indice\n",
        "\n",
        "#X = scaler.fit_transform(blobs[0])\n",
        "\n",
        "#Creamos un arreglo para almacenar los posibles valores\n"
      ],
      "metadata": {
        "id": "v1nVQo1yGX3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Ingresamos número de clusters 'n_clusters'\n",
        "#Definimos cuantas veces se repite el proceso n_init = 10 por defecto\n",
        "\n",
        "#Entrenamos el modelo\n",
        "\n",
        "#Obtenemos las etiquetas de pertenencia\n",
        "\n",
        "#Graficamos el resultado\n"
      ],
      "metadata": {
        "id": "TqnwyedBv5X5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}